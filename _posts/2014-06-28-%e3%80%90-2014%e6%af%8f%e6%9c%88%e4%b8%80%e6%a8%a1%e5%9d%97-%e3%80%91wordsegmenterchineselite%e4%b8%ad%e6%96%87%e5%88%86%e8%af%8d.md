---
title: 【 2014每月一模块 】Word::Segmenter::Chinese::Lite中文分词
date: 2014-06-28T16:37:14+00:00

---
前段时间给聊天室写叶贝斯过滤的时候，发现CPAN上居然没有中文分词模块。
  
好吧，我来贡献一个。
  
支持一元分词，二元分词，词典分词。
  
词典是内嵌的。不用大家去找了。

地址在此：[Word::Segmenter::Chinese::Lite](https://metacpan.org/pod/Word::Segmenter::Chinese::Lite "Word::Segmenter::Chinese::Lite")

用法依然简单：

<pre class="brush: perl">use Word::Segmenter::Chinese::Lite qw(wscl_seg wscl_set_mode);

# 默认使用词典分词 
my @result = wscl_seg("中华人民共和国成立了oyeah");
foreach (@result)
{
  print $_, "\n";
}
# got:
# 中华人民共和国
# 成立
# 了
# oyeah
 
# 交叉二元分词
wscl_set_mode("obigram");
my @result = wscl_seg("中华人民共和国成立了");
foreach (@result)
{
  print $_, "\n";
}
# got:
# 中华
# 华人
# 人民
# 民共
# 共和
# 和国
# 国成
# 成立
# 立了
# 了
 
# 一元分词
wscl_set_mode("unigram");
my @result = wscl_seg("中华人民共和国");
foreach (@result)
{
  print $_, "\n";
}
# got:
# 中
# 华
# 人
# 民
# 共
# 和
# 国
</pre>

关于分词本身，没啥可说的，这次扯两句闲篇吧。

现在ES很火，以前lucene很火。其实多对于很小应用（比如说我们4000万日PV网站的搜索）来说，它们都过于杀鸡牛刀了。自己实现其实更加轻松写意。
  
最关键的是，牛刀在大多数时候，并杀不好鸡。

曾经GFS很火，现在golang又很火，凡是Google挺的东西都会很火。
  
曾经lucene很火，现在hadoop又很火，Doug Cutting的东西都会很火。
  
其实我想说lucene的实现并不咋地，性能也相当一般。
  
GFS在为我们所知的时候其实也就那样。Google放论文出来时，那架构已经并不先进了。
  
过两年，Google自己就放弃GFS了。可一堆舔臭脚的还在玩命捧。
  
现在的hadoop其实也一样。

我就不明白，干嘛总会有所谓“热门的技术”。
  
这些技术的火爆，到底是因为真的精准命中所有人的业务，还是因为Google和Apache主席双重爆点导致的呢？
  
干嘛总会有“用xxx软件就是所有人干xxx事不约而同的解法”。
  
难道咱们的世界里，需求同质化真有如此严重？

我就很烦这些事。其实太多东西有更简单解法。

以上就是今天凑字数的内容。
