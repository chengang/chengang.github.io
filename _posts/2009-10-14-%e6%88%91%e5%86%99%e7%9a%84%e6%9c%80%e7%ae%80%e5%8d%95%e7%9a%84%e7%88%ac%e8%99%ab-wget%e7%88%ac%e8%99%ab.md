---
id: 1296
title: '我写过最简单的爬虫&#8211;wget爬虫'
date: 2009-10-14T18:06:26+00:00
author: chen
layout: post
guid: http://blog.chengang.cc/?p=1296
permalink: /1296.html
categories:
  - 实践
tags:
  - bash
  - spider
---
wget正经是个好爬虫，不仅有个spider参数，还会自动去搜寻robots.txt。
  
托工作的福，把它的man全部浏览了一遍，今后wget也装进我的工具箱了。
  
以下是我写过的最简单的爬虫(一次比一次的简单啊..) ，用了16个参数。

<pre class="brush: bash"># 2009.10.14 @ v.sina.com.cn
wget --protocol-directories -4 
     -r -l 2 
     -H -N -D "v.sina.com.cn,video.sina.com.cn,ent.sina.com.cn,news.sina.com.cn" 
     --exclude-domains "search.video.sina.com.cn" 
     -R "*.jpg,*.gif,*.png,*.css,*.pdf,*.doc,*.flv,*.hlv" 
     -A "*.htm,*,html,*shtml,*.php" 
     -T 3 -t 1 
     -U "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; GTB5)" 
     -b -nv -a "downloads.log"
     http://video.sina.com.cn/life/ http://video.sina.com.cn/movie/ http://video.sina.com.cn/live/ http://video.sina.com.cn/news/ http://v.sina.com.cn/</pre>

还可以把-N参数（验证last_modify头）换成-nc参数（不重复下载已存在文件），然后同时启动好几个wget来加快下载速度。

但是wget也有一个弱点，我还没找到解决方法——

面对类似

<pre class="brush: jscript">if(commUrl){
    //优化评论数读取体验
    var comNum = document.getElementById("commentNum").innerHTML;
    linkHTML += '&lt;a href="' + commUrl + '" target="_blank"&gt;[评论 <span id="commentNum" style="color:#f00">' + comNum + '</span>条]&lt;/a&gt;';
    //081119001 ws end
};</pre>

这样的js代码，它会误把其中的不合法url（<a href=&#8221;&#8216; + commUrl + &#8216;&#8221; target=&#8221;_blank&#8221;>）解析出来并尝试下载，即使我设置了 &#8211;ignore-tags=script,style。