---
title: RHCA培训散记（436存储与高可用集群 – 3）《关于RAID》
date: 2011-12-07T01:37:04+00:00

---
## RAID

**1、软RAID vs 硬RAID**：
  
a> 软RAID是一个OS中物理磁盘和逻辑磁盘之间的一个抽象层，这个抽象层会耗费一些CPU资源。硬RAID则没有这个问题；
  
b> 硬RAID可以支持热拔插磁盘，这样带来的好处是可以在线换掉损坏的磁盘。但新一代的SATA软RAID也可以支持热拔插（拔之前先要命令行移除）；
  
c> 硬RAID通常还有一些厂商额外提供的功能；
  
d> 硬RAID需要专门的独立设备，这意味着多付出钱；

**2、半硬半软RAID**，除了软RAID和硬RAID还有这种一种存在。它是把IO工作交给CPU去完成，它们一般不支持RAID5；

**3、组软RAID时要注意mkfs的block值和步进值**（后有详述）；

**4、RAID 0**：
  
a> 非嵌套式RAID中性能最好的；
  
b> 并非只是2块盘可以组RAID 0，三块或更多都可以，但是性能是有边际效应的（也就是说，假设一个软盘的性能是50MB每秒，两个软盘的RAID 0性能约96MB每秒，三个软盘的RAID 0也许是130MB每秒而不是150MB每秒）；
  
c> 容量 ＝ 最小那块的磁盘的容量 x 磁盘数（有一些软件RAID可以做到不受此限制，linux的实现就可以），存储效率100%；
  
d> 一块盘坏，所有数据完蛋；
  
e> 注意：读性能并不会得到写性能那么样大的提升，主要提升的是写性能；
  
f> 应用场景：非关键数据、不经常写入的而又需要高速写入性能的备份数据；

**5、RAID 1**：
  
a> 镜像。可以由2个以上磁盘组成，数据被复制到组里的每一块盘，只要还有一块没坏，数据就不丢失，可靠性最强；
  
b> 只算阵列中最小的那块盘的容量，存储效率（100/N）%，N为阵列中磁盘的数量。是所有RAID中利用率最低的；
  
c> 最少由2块，或者更多的磁盘组成（建议2块）；
  
d> 写性能微小降低，读性能大幅增强（如果是软RAID需要多线程操作系统才能增强，如Linux）；
  
e> 组内的磁盘需性能相近（最好是一样的盘），这样负载才均衡；
  
f> 可以作为一种热备份的方案（往RAID 1中加入磁盘，同步好后拔下来就是一个备份盘了）；
  
g> 应用场景：要求高冗余，而对成本、性能、容量不那么敏感的场景；

**6、RAID 2**：
  
a> RAID 0的改良版；
  
b> 最少3块磁盘才能组成，
  
c> 将数据编码（Hamming Code）后分散到每块盘；
  
d> 加入ECC校验码，所以要比RAID 0多耗费一些磁盘空间；

**7、RAID 3**：
  
a> 也是将数据编码后分散到不同的物理磁盘；
  
b> 使用Bit－interleaving（数据交错存储）的方式将数据分散；
  
c> 将可用于恢复的校验值写入单独的另一块磁盘；
  
d> 由于使用bit分区，每读一点数据都可能需要开动读所有的磁盘，因此很适合大数据量的读取；

**8、RAID 4**：
  
a> 大部分是和RAID 3一样的；
  
b> 不同的地方是它使用Block－interleaving而非Bit－interleaving做数据分区；
  
c> 无论读写什么哪块磁盘上的数据，校验盘均需要被读写，因此这块校验盘的负载明显高于其它盘，因此校验盘十分容易损坏；

**9、RAID 5**：
  
a> 使用Disk Striping进行数据分区。分区的方式有左对称、左非对称、右对称，RHEL默认选择左对称，因为少一次寻址，这是读性能最高的选择；
  
b> 不存储数据副本，而是存储一个校验值（比如说使用XOR，实际也是XOR），当数据块丢失，根据剩下的同组的数据块和校验值来恢复数据；
  
c> 校验值被分散到每一块磁盘而非单独一块磁盘上，这样避免了某一块磁盘的负载明显高于或低于其他盘；
  
d> 至少需要2块盘来组成，性能和冗余性都有一定提升，是RAID 0和RAID 1的折中选择；
  
e> 存储利用率很高，为100 x (1 &#8211; 1/N)%，N为物理磁盘数量，相当于耗费一块盘的容量，但是这个耗费的容量是分散在每块盘里的，而不是某一块物理盘；
  
f> 性能可以通过调整条带大小来调解（分配读写条带）；
  
g> 坏掉1块盘，还可以重建数据，但由于重建（degraded）数据需要通过校验值计算得出，因此重建过程中性能会急剧下降。且重建是个较长的过程；
  
h> 应用场景：较高的读性能要求，较少的写性能要求，低成本的考虑；

**10、RAID5的写入性能很差**，因为每次写入实际都将触发4次（以上）的I/O，虽然cache机制会减小实际的I/O开销。过程如下：
  
a> 即将被写入的数据从磁盘中被读出；
  
b> 写入数据，但此时校验值还未更新；
  
c> 把同组所有块的数据都读进来，把校验块读进来；
  
d> 计算校验值并将新校验值写回校验块；

**11、RAID 6**：
  
a> 包括分区在内的各种机制和RAID5基本相同；
  
b> 不同在于它会保存2个由不同算法（课本上说是不同的分组方式，如1－3算值1、2－4算值2）算出的校验值，这样就会消耗2块盘的容量（也是分散在各个盘的）；
  
c> 最少由4块盘组成，允许同时坏2盘；
  
d> 冗余程度比RAID 5更高，同时写入性能比RAID 5更烂（事实上由于会引发很多次实际I/O，非常烂）；
  
e> 存储利用率为100 x (1 &#8211; 2/N)%，N为物理磁盘数量；
  
f> 重建速度比RAID 5块，重建时性能优于RAID 5，重建时失败的风险比RAID 5低（RAID 5在重建过程中由于压力很大容易再坏掉一块盘）；
  
e> 应用场景：需求比RAID 5更高的冗余保护，需求较高读性能较低写性能，低成本的考虑；

**12、RAID 7**：
  
RAID 7并非公开的RAID标准，而是Storage Computer Corporation的专利硬体产品名称，RAID 7是以RAID 3及RAID 4为基础所开发，但是经过强化以解决原来的一些限制。另外，在实作中使用大量的高速缓存记忆体以及用以实现异步阵列管理的专用即时处理器，使得RAID 7可以同时处理大量的IO要求，所以性能甚至超越了许多其他RAID标准的实作产品。但也因为如此，在价格方面非常的高昂。（直接摘自http://zh.wikipedia.org/wiki/RAID#RAID_7）

**13、RAID 10**：
  
a> 嵌套式RAID，可细分为1＋0和0＋1；
  
b> 需要至少4块磁盘组成，存储效率为(100/N)%，N为镜像数；
  
c> 性能和冗余兼顾，就是利用率稍低；
  
d> 创建软RAID 10时一般先创建2个RAID 1，然后在此基础上创建RAID 10；

**14、嵌套式RAID**一般还有RAID 50和RAID 53；

**15、较少实际应用的RAID**是RAID2、3、4，因为RAID5已经涵盖了所需的功能。因此RAID2、3、4大多只在研究领域有实现，而实际应用上则以RAID5或RAID6为主。

**16、RAID DP**（dual parity）是NetApp公司设计的系统，是使用RAID4（而非传说的RAID 6，因为它的校验盘是独立的）的设计概念，尽量每次只与2块盘打交道。此种RAID和RAID 6一样也允许同时坏2块盘，它们开发多年的文件系统WAFL为RAID DP做了专门优化，效率高于RAID 6；

**17、优化RAID参数**（条带化参数）：
  
a> 调整条带化参数对RAID有很大的影响，适用于RAID 0、RAID 5、RAID 6；
  
b> chunk-size。RAID写满一个chunk size才会移动到下一块磁盘去，有时候它直接就是条带化的粒度。chunk size应该是block size的整数倍；
  
c> 减小chunk-size意味着文件会分成更多片，分布在更多的物理磁盘上。这样会提高传输效率，但是可能会降低定位寻道的效率（有些硬件实现会等填满一个条带才真正写入，这样会抵消一些定位寻道的消耗）；
  
d> 增大chunk-size会得到以上相反的效果；
  
e> stride（步进值）是类ext2文件系统的一个值，用于在类ext2数据结构里seek，它这样指定：mke2fs -E stride=N。N（也就是步进值）应该被指定为 chunk-size / filesystem block size（文件系统的block size）。（mke2fs -j -b 4096 -E stride=16 /dev/md0）
  
f> 以上2个值调校对了，可以提高RAID的在多个磁盘上的并行效率，从而让整个RAID的性能随物理磁盘数量提升而提升；

**18、更多RAID原理**可见http://blog.chinaunix.net/space.php?uid=20023853&do=blog&id=1738605；

**19、软RAID的信息**可在系统中以下几个地方查看：
  
a> /proc/mdstat。这里方便看新建、重建的进度条，以及哪块盘坏了等；
  
b> mdadm &#8211;detail /dev/md0。显示详细信息；
  
c> /sys/block/mdX/md。这里方便看到cache_size等信息（别处没有的）；

**20、Major/Minor号**。linux系统中前者代表设备类型／驱动，后者是同种类型／驱动的多个设备在系统中的ID号；

**21、RAID技术最初**由IBM公司而非贝尔实验室发明（http://en.wikipedia.org/wiki/RAID#History）；

**22、RAID是支持在线扩容的**；

**23、硬盘出场基本都是有坏道的**，但是厂商会预留一部分磁道（而且通常在外圈）出来以顶替坏道消耗的容量。相较而言，企业级硬盘预留的容量较家用级硬盘大，这是它更贵寿命更长的原因之一；

**24、RHEL下软RAID的配置文件**（用于开机自动发现RAID）在/etc/mdadm.conf。此文件可以这么生成：mdadm &#8211;verbose &#8211;examine &#8211;scan > /etc/mdadm.conf。此文件中还需配置RAID坏了时如何mail通知管理员；

**25、Critical Section**中存储了RAID中数据的MeteData。它是元数据，如果它丢了，RAID中的数据就全丢了。各种RAID中，只有RAID 1的Critical Section是写在磁盘尾上的（其它都写在磁盘头），因此RAID 1较为适合做系统盘（易恢复MeteData）。

**26、软RAID重组**（reshape）（比方说往里加盘，或是更改chunk size或RAID级别）时，如果想保留原数据，则备份Critical Section十分重要。重组的内部过程是这样的：
  
a> 将RAID设为只读；
  
b> 备份Critical Section；
  
c> 重定义RAID；
  
d> 成功的话删除原Critical Section，恢复可写；

**27、最好手工指定Critical Section备份位置**（最好备份到将要改变的RAID之外的地方），因为如果reshape失败Critical Section是需要手工恢复的。如果没有手工制定位置，Critical Section会被存在RAID的备盘上，如果RAID没有备盘则会存放在内存中（易丢失，危险）；

**28、如果RAID改变了**（比如拉伸），记得之上的filesystem也要做出相应的改变（比如说拉神）；

**29、系统启动脚本rc.sysinit中启动服务的顺序**默认是这样的：udev->selinux->time->hostname->lvm->软RAID。所以在lvm上做RAID比较合理，除非手动改启动脚本；

**30、扩大RAID 5的容量**，可以通过逐步把RAID 5中的每块硬盘换成更大容量的盘的方式来完成（别忘记resize filesystem）；

**31、同台机器上不同的软RAID组间可以共享备盘**；

**32、如果要在机器间迁移软RAID**，记得迁移之前要把要迁移的RAID改成一个目标机器上不存在（也就不冲突）的RAID设备名（mdX）。改名的时候可以用Minor号来指认要改名的RAID；

**33、Bitmap用于记录本块物理磁盘的每个block是否跟整个RAID同步**着呢。它会被周期性地写入。启用它可以极大加快RAID的恢复过程；

**34、Bitmap可以存放在RAID内或RAID外**。如果告诉RAID把Bitmap存在RAID外，给的绝对路径又是在RAID内的，RAID会死锁。如果存在RAID外的话，只支持extX的文件系统；

**35、在活动的RAID上加Bitmap的前提**是它的Superblock是好的；

**36、可以启用Write Behind机制**，如果启用了Bitmap的话。这样就会在指定磁盘成功写入后就会给应用程序返回成功，另外的盘会异步写入数据，这对RAID 1特别有用；

**37、RAID机制不会去主动检测坏块**。只有在被迫读到坏块读不出来时它才会去尝试修复（找备块替代），要是那磁盘修复不了了（备块用光了），那就会把那块盘标志为错误，然后踢出RAID，启用备盘；

**38、因为RAID对坏块是消极处理的，所以在RAID 5重建的过程中就很有可能发现隐藏的坏块，从而导致RAID恢复失败**。磁盘容量越大，此风险越大；

**39、用crontab定期检查一下RAID坏块是个不错的建议**。在Kernel 2.6.16后我们可以这样触发检查：echo check >> /sys/block/mdX/md/sync_action。
