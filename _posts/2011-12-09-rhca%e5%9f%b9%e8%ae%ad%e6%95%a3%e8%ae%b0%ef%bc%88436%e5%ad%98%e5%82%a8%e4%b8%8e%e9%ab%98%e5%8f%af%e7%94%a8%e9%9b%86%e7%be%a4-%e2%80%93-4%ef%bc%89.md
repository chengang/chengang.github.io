---
title: RHCA培训散记（436存储与高可用集群 – 4）
date: 2011-12-09T14:38:59+00:00
layout: post
---
## 高可用集群

**1、高可用的目标**：
  
a> 要有能力监控用于服务的资源和组件的状态；
  
b> 任何一点失败后，整个服务要在短时间内恢复可用状态；
  
c> 有能力自动修复简单的资源或组件的失败；
  
d> 服务可配置以及报警；

**2、集群的目标**：
  
a> 伸缩性；
  
b> 配置简单；
  
c> 资源可动态调配；

**3、红帽的解决方案Red Hat Cluster Suite**（红帽集群套件），使用集群来实现高可用。它是由各领域内最优秀的软件集成的，包含以下组件：
  
配置相关：
  
a> ccsd，配置管理（替代了OpenAIS内置的配置管理）。配置文件里面会有整个集群的概况，每个节点上都有一份。节点会自动寻找整个集群中最新（最高版本）的配置文件并同步到本节点（quorate状态下）；

集群相关：
  
b> cman，负责投票以及加入退出集群；
  
c> aisexec，负责OpenAIS集群管理：成员间通信、加密；
  
d> rgmanager，资源管理，控制尝试修复不可用资源和彻底不可用资源的故障转移（设计为资源的冷转移，即重启服务；热转移需要被集群的应用程序作出修改）；

共享存储相关：
  
e> fenced，负责fencing机器防止不当IO；
  
f> DLM，分布式锁管理；
  
g> clvmd，分布式逻辑卷管理；

部署相关（用web的方式解决GUI配置和部署）：
  
h> luci，用于部署的控制端，一般独立于集群外（可以集中查看节点日志）；
  
g> ricci，用于部署的agent，在节点上；

**4、思考：如果不用多播，如何发现新加进集群的成员**。试答：在集群内的某一个成员的配置里写上新成员的信息，这个工作可以通过类似luci的web控制端自动完成。

**5、觉得RHCS不易用且不可靠**：
  
a> 老师说fence若拿不到返回值则整个集群都会处于挂起状态（我觉得如果真是这样，那应该算是BUG吧）；
  
b> 需要手动配置/etc/hosts（这一点真烂啊）；
  
c> 对节点重启速度敏感；

**6、RHCS需要多播**，如果硬件设备不支持多播，RHCS自动会使用广播来通信；

**7、RHCS觉得共享存储很重要**，我不这样想。集群内的p2p通信要是完善的话，交换点数据根本不成问题。这只是为了简化设计使用而多播造成的不得不承担的后果之一（后面它也觉得不对，于是错上加错又搞出了GFS这么个有复杂性能又有局限的东西）；

**8、RHCS部署节点的时候只用部署一个ricci，然后一切它会代理完成，这设计很好。**以后我的集群也要往这个目标努力，我觉得易于部署是良好设计的外在重要表现之一；

**9、OpenAIS**，是RHCS在RHEL5之后采用的集群消息框架:
  
a> 它的核心消息传递系统叫做totem，加密传输；
  
b> 开发社区很活跃；
  
c> 能快速检测节点失败；
  
d> ）大多数时候跑在用户态，不会造成kernel panic，也容易调试；
  
e> 高级的，精致的消息协议和集群成员管理模型；

**10、Quorum的主要目的**是通过投票机制防止网络脑裂（那假设网络不脑裂，这么复杂的设计是不是就不必要了呢？），节点加入或者离开都会触发投票；

**11、心跳是通过多播或者广播发送的**；

**12、需要有简易接口查询Quorum状态**，RHCS中是clustat －Q；

**13、手动更改Quorum票数（expect votes）可能会导致脑裂**；

**14、高可用集群方案还有**：
  
a> IBM：HACMP；
  
b> heartbeat（Pacemaker）；
  
c> SteelEye：LifeKeeper
  
d> NEC：ExpressCluster
  
e> ROSE HA
  
f> HAProxy

**15、只有2个节点时无法防止脑裂**；

**16、配置文件的正确更新依赖配置ID的单调递增**；

**17、RHCS的配置文件**（其实是集群信息文件）中有定义超过200个属性；

**18、cman_tool**用于手动管理节点的加入离开以及杀掉别的节点，它也提供了API；

**19、带内fencing不可靠**（但我觉得依然可以尝试一下的，自动重启嘛，很多时候还是能有效果的）；

**20、fencing分为2种：掐断通信链路和掐断电源。**红帽建议掐断电源，因为掐断链路的话，等链路恢复了错误依然有可能发生（因为重发机制，等在那了）；

**21、fencing的目的**是为了保护共享存储，所以集群使用共享块设备是原罪啊，会带来这么大的麻烦呢；

**22、fencing需要有权限控制**；

**23、大多数网络管理插座同一时刻只允许一个人登陆进来**，要是同时有2个节点需要fencing那就很麻烦，所以每次fencing的时候都要快近快出以防止冲突（脚本要写好，注意及时释放资源）。你看，多麻烦；

**24、不起fenced进程的节点不允许加入GFS**；

**25、fenced依赖**cman告诉它什么时候需要fencing谁，依赖ccs告诉它如何去实现fencing；

**26、集群没有quorum时不会fencing任何人。**如果fencing会让集群失去quorum，那么fencing也不会执行；

**27、RHCS支持对同一个节点配置多个fencing机制**；

**28、RHCS并不能保证数据完全安全**，比如说一个节点的网络和它的fencing设备同时坏掉了。那数据就糟糕了；

**29、关于failover域**讲了很多，功能是强大，但我觉得这些对一个单独的服务而言不大用得上；

**30、如果节点间共享NFS的话，mount NFS的时候要为每一个节点手工设置同样的fsid**，这样才能确保它们使用相同的文件句柄（又是一个共享存储的麻烦）；

**31、应该具有手工在节点之间迁移服务的工具**，RHCS中是clusvcadm；

**32、RHCS支持的资源类型**：GFS、ext2、ext3、IPAddr、NFS、Script、Samba、Apache、LVM、MySQL、OpenLDAP、PstgreSQL、Tomcat。其实还蛮有限的⋯⋯；

**33、RHCS的概念是资源组成服务，它通过检测每个资源来监控整个服务**。这样的坏处是不准（各个资源好，其实整个服务未必好）且复杂（还要关心资源之间的依赖关系），好处是可以针对具体失败的资源尝试自动恢复；

**34、网卡状态可以通过ethtool、mii-tool监控**；

**35、RHCS检查每个资源的间隔默认是30s，最小能设置到5s**。课本上有些资源检查很耗时，举例是Orcale；

**36、在节点上更改整个集群时，先要检查此节点有没有quorum**；

**37、RHCS在节点间切服务还是要花上好几秒的**；

**38、集群这种复杂的玩意儿，各种工具一定要全**，方便调错；

**39、服务的状态**有：Started、Pending、Disabled、Stopped、Failed；

**40、红帽想把集群状态放进SNMP里去**，那样显然方便很多。不过几年过去了，它们始终没有成功（我也觉得放进去并不十分合理）；

**41、如果想关掉RHCS**（可能搬家啥的），那不能直接关机，因为那样会导致失去quorum，那剩下的机器就啥都干不了了。可以手动调低quorum，或者手动将节点们一台一台leave出集群；

**42、把集群节点自己的日志和节点的系统日志放打一起会是个好主意**，因为节点的物理机器深受集群控制，一旦节点失败就可能要查询各方面的原因；

**43、quorum disk**。我想我没弄懂这玩意儿！网上查了些资料只知道是IBM首先发明的，属于PAXOS的扩展，主要用于双机集群，会往里写心跳信息，写心跳之前能用脚本自我检测，还能达到检测共享存储的目的。_关于设计思想和原理，都是语焉不详。要是我哪天真搞明白了，我再单独写一篇吧。_下一条简述了——

**44、quorum disk注**。2012－12－19，关于qdisk问了几个用过的家伙，大概意思是这样的：qdisk只投一次票，只给一个子集群投票，这样就防止了集群的脑裂。qdisk一般建立在共享存储上，这是因为共享存储的链路一般被假设为较为可靠的链路。所以说qdisk存在的目的主要有2：a> 防止偶数个节点的集群发生脑裂；b> 增加整个集群的投票总数，这样可以防止有一些节点down掉以后整个集群失去quorum的情况，也就是说让集群在只有不多的几台机器下依然保持可以服务。
